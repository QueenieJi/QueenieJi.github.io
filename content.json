{"pages":[{"title":"Project","text":"","link":"/project/index.html"}],"posts":[{"title":"Foundation of Concurrency","text":"There are some learning notes about UNSW COMP3151/9154. Main contents are based on Ben Ari’s book Principles of concurrent and distributed programming algorithms and models and lecture slides. Final RevisionLinear Temporal Logic (LTL)A linear temporal property is a set of behaviours. LogicDefinition: A logic is a formal language designed to express logical reasoning. Like any formal language, logics have a syntax and semantics. In concurrency:take time into account SemanticsSemantics are a mathematical representation of the meaning of a piece of syntax. There are many ways of giving a logic semantics, but we will use models. LTLLinear temporal logic (LTL) is a logic designed to describe linear time properties. Modal or temporal operators If $φ$ is an LTL formula, then $\\bigcirc φ$ is an LTL formula.If $φ, ψ$ are LTL formulae, then $φ U ψ$ is an LTL formula. circle: nextU: untildescribe a behaviour $\\bigcirc$No Orange -&gt; it’s not orange right now$\\bigcirc$ Orange -&gt; it’s orange in the next state Uorange U blue -&gt; at some point in the future (don’t have to hold forever and the future can be now) , the right will be true, before that, the left is true LTL SemanticsLet $σ = σ_0σ_1σ_2σ_3σ_4σ_5$ . . . be a behaviour. Then define notation: $σ|_0 = σ$ $σ|_1 = σ_1σ_2σ_3σ_4σ_5 . . . $ $σ|_{n+1} = (σ|_1)|_n$ Semantics The models of LTL are behaviours. For atomic propositions, we just look at the first state. We often identify states with the set of atomic propositions they satisfy. $σ\\vDash p ⇔ p∈σ_0$ $σ\\vDash φ∧ψ ⇔ σ \\vDash φ$ and $σ\\vDashψ$ $σ\\vDash¬φ ⇔ σ\\not\\vDashφ$ $σ|=\\bigcirc φ ⇔σ|_1\\vDashφ$ $σ|=φ U ψ ⇔ $ There exists an $i$ such that $ σ|_i \\vDashψ$ ​ and for all $j &lt; i$, $σ|_j \\vDash φ$ We say $P \\vDash φ$ iff $\\forall \\varphi \\in P$ and $σ \\vDash φ$. Derived Operator The operator $\\Diamond φ$ (“finally” or “eventually”) says that $φ$ will be true at some point. The operator $\\Box \\varphi$ (“globally” or “always”) says that $\\varphi$ is always true from now on. operator diamond =&gt; eventually$\\Diamond φ=$ T $U φ$we don’t care about the left hand side =&gt; True operator box =&gt; for any time from now on, phi will hold $\\Box \\varphi = \\neg \\Diamond \\neg φ$ Exercise: Infinitely Often= always eventually phi= square diamond phi Safety/Liveness A safety property states that something bad does not happen. These are properties that may be violated by a finite prefix of a behaviour. A liveness property states that something good will happen. These are properties that can always be satisfied eventually Kripke Structures (KS)These traces that we have examined are still sequences of actions, not states. Behaviours, however, are sequences of states. Normally, to convert our labelled transition systems into something we can reason about in LTL, we first translate them into an automata called a Kripke Structure. A Kripke structure is a 4-tuple $(S,I,↦,L)$ which contains a set of states $S$, an initial state $I$, a transition relation $↦$ which, unlike a labelled transition system, does not have any action labels on the transitions, and a labelling function $L$ which associates to every state $S$ a (set of) atomic propositions – these are the atomic propositions we use in our LTL formulae. A Kripke structure for an OS process behaviour was actually shown in the lecture on temporal logic in Week 1, I just never told you it was a Kripke structure. Kripke Structures deal with states, not transition actions. This means, to translate from a labelled transition system to a Kripke structure, we need a way to move labels from transitions to states. The simplest translation is due to de Nicola and Vaandrager, where each transition $s_i\\xrightarrow[]{a}s_j$ in the LTS is split into two in the Kripke Structure: $s_i→X$ and $X→s_j$, where X is a new state that is labelled with a. Because this converts existing LTS locations into blank, unlabelled states in the Kripke structure, this introduces problems with the next state operator in LTL. For this reason (and others) we usually consider LTL without the next state operator in this field. Normally with LTL, we require that Kripke structures have no deadlock states, that is, states with no outgoing transitions. The usual solution here is to add a self loop to all terminal states. We can extract our normal notion of a behaviour by using the progress completeness criterion. Because of the restriction above, the progress criterion is equivalent to examining only the infinite runs of the automata. Calculus of Communicating Systems (CCS)CCSThe Calculus of Communicating Systems: Is a process algebra, a simple formal language to describe concurrent systems. Is given semantics in terms of labelled transition systems. Was developed by Turing-award winner Robin Milner in the 1980s. Has an abstract view of synchronization that applies well to message passing ProcessesProcesses in CCS are defined by equations: Example: The equation: CLOCK = tick defines a process CLOCK that simply executes the action “tick” and then terminates. This process corresponds to the first location in this labelled transition system (LTS):$$\\begin{align}&amp;\\bullet\\\\&amp;\\Bigg\\downarrow\\text{tick}\\\\&amp;\\bullet\\end{align}$$An LTS is like a transition diagram, save that our transitions are just abstract actions and we have no initial or final location. Action PrefixingDefinition If a is an action and P is a process, then $x.P$ is a process that executes $x$ before $P$. This brackets to the right, so: $$x.y.z.P = x.(y.(z.P))$$ Stopping More precisely, we should write: CLOCK2 $=$ tick.tock.STOP where STOP is the trivial process with no transitions. LoopsUp to now, all processes make a finite number of transitions and then terminate. Processes that can make a infinite number of transitions can be pictured by allowing loops: CLOCK4 = tick.CLOCK4 CLOCK5 = tick.tock.CLOCK5 We accomplish loops in CCS using recursion. Equality of ProcessesInformal Definition We consider two process to be equal if an external observer cannot distinguish them by their actions. We will refine this definition later These two processes are physically different: But they both have the same behaviour — an infinite sequence of “tick” transitions ChoiceDefinition If P and Q are processes then P + Q is a process which can either behave as the process P or the process Q. Choice Equalities Observe that we have the following identities about choice:$$\\begin{align}&amp;P + (Q + R) = (P + Q) + R &amp;\\text{(associativity)}\\\\&amp;P + Q = Q + P &amp;\\text{(commutativity)}\\\\&amp;P + \\textbf{STOP} = P &amp;\\text{(neutral element)}\\\\&amp;P + P = P &amp;\\text{(idempotence)}\\\\\\end{align}$$ Bisimulation equivalenceOur notion of equality without this equation is called (strong) bisimulation equivalence or (strong) bisimilarity Partial trace equivalenceWe examined the two CCS processes $a⋅(b+c)$ and $a⋅b+a⋅c$ and concluded that LTL would not be able to distinguish these two processes, even though they are not considered equal (bisimilar) in CCS. If we were to try to add the equation $a⋅(P+Q)=a⋅P+a⋅Q$ into CCS, then we would identify those two processes above, but we would also identify $a⋅b+a$ with $a⋅b$, and clearly one of these satisfies $◊b$ and the other doesn’t! This is because the semantic equivalence we get by adding that equation is called partial trace equivalence. As mentioned in lectures, partial trace equivalence is like looking at all the possible sequences of actions (traces) in the process, including those sequences that do not take available transitions. So, the traces of $a⋅b+a$ are $ϵ$ (the empty trace), $a$, and $ab$ – exactly the same as the partial traces of $a⋅b$. Because partial trace equivalence includes these incomplete traces, using it is basically giving up on the vital progress assumption. Progress is the assumption that a process will take an action if one is available. Parallel CompositionIf P and Q are processes then P | Q is the parallel composition of their processes — i.e. the non-deterministic interleaving of their actions SynchronizationIn CCS, every action a has an opposing coaction $\\bar a$ (and $a$ = $\\bar {\\bar a}$) It is a convention to think of an action as an output event and a coaction as an input event. If a system can execute both an action and its coaction, it may execute them both simultaneously by taking an internal transition marked by the special action $τ$ Expansion TheoremLet $P$ and $Q$ be processes. By expanding recursive definitions and using our existing equations for choice we can express $P$ and $Q$ as n-ary choices of action prefixes:$$P =\\Sigma_{i \\in I} αi. Pi \\text{ and } Q =\\Sigma_{j∈J}βj.Qj.$$ Then, the parallel composition can be expressed as follows: $$P | Q = \\Sigma_{i∈I} αi .(P_i | Q) + \\Sigma_{j∈J} βj .(P | Q_j) + \\Sigma_{i∈I, j∈J, αi=\\bar β_j} τ.(P_i | Q_j).$$ RestrictionIf P is a process and a is an action (not $τ$ ), then $P$ \\ $a$ is the same as the process P except that the actions $a$ and $\\bar a$ may not be executed. We have$$(a.P) \\backslash b = a.(P \\ b) \\text{ if } a \\not ∈ {b, \\bar b}$$ Inference RulesIn logic we often write:$$\\frac{A_1\\qquad A_2\\qquad …\\qquad A_n}{C}$$To indicate that $C$ can be proved by proving all assumptions $A_1$ through $A_n$. For example, the classical logical rule of modus ponens is written as follows:$$\\frac{A\\implies B \\qquad A}{B} \\text{MODUS PONENS}$$ Operational Semantics$$\\begin{align}&amp;\\frac{}{a.P\\xrightarrow[]{a}P}\\text{ACT}&amp;\\frac{P\\xrightarrow[]{a}P’}{P + Q\\xrightarrow[]{a}P’}\\text{CHOICE}_1&amp;\\frac{Q\\xrightarrow[]{a}Q’}{P+Q\\xrightarrow[]{a}Q’}\\text{CHOICE}_2\\\\&amp;\\frac{P\\xrightarrow[]{a}P’}{P|Q\\xrightarrow[]{a}P’|Q}\\text{PAR}_1&amp;\\frac{Q\\xrightarrow[]{a}Q’}{P|Q\\xrightarrow[]{a}P|Q’}\\text{PAR}_2&amp;\\frac{P\\xrightarrow[]{a}P’\\qquad Q\\xrightarrow[]{\\bar a}Q’}{P|Q \\xrightarrow[]{τ}P’|Q’}\\text{SYNC}\\\\&amp;&amp;\\frac{P\\xrightarrow[]{a}P’\\qquad a\\not \\in {b, \\bar b}}{P\\backslash b \\xrightarrow[]{a}P’\\backslash b}\\text{RESTRICT}\\end{align}$$ Bisimulation EquivalenceTwo processes (or locations) P and Q are bisimilar iff they can do the same actions and those actions themselves lead to bisimilar processes. All of our previous equalities can be proven by induction on the semantics here. Proof Trees Value PassingWe introduce synchronous channels into CCS by allowing actions and coactions to take parameters.$$\\begin{align}&amp;\\text{Actions}: &amp;a(3)&amp; &amp;c(15)&amp; &amp;x(\\text{True}) . . . \\\\&amp;\\text{Coactions}: &amp;\\bar a(x)&amp; &amp;\\bar c(y)&amp; &amp;\\bar c(\\bar z) . . .\\end{align}$$The parameter of an action is the value to be sent, and the parameter of a coaction is the variable in which the received value is stored. Example: A one-cell sized buffer is implemented as:$$\\textbf {BUFF} = \\overline{\\text{in}}(x).\\text{out}(x).\\textbf{BUFF}$$Larger buffers can be made by stitching multiple BUFF processes together! This is how we model asynchronous communication in CCS. Merge and GuardsTo do a deterministic (fair) merge, we would need some way to check if one value is larger than another. Rather than add if statements, we add the notion of a guard Definition If $P$ is a value-passing $CCS$ process and $\\varphi$ is a formula about the variables in scope, then [$\\varphi$]P is a process that executes just like P if $\\varphi$ is holds for the current state and like STOP otherwise. We can define an if statement like so: if $\\varphi$ then P else Q ≡ ([$\\varphi$].P) + ([¬$\\varphi$].Q) AssignmentDefinition If P is a process and x is a variable in the state, and e is an expression, then $[![x := e]!]$ P is is the same as P except that it first updates the variable x to have the value e before making a transition. LTS Labelled Transition SystemsTransition DiagramsDefinition A transition diagram is a tuple (L,T,s,t) where: L is a set of locations (program counter values). s ∈ L is a entry location. t ∈ L is a exit location. T is a set of transitions. A transition is written as $l_i \\xrightarrow[]{\\text{g;f}} l_j$ where: $l_i$ and $l_j$ are locations. g is a guard Σ → B (state to boolean) f is a state update Σ → Σ. (State to state) Floyd VerificationRecall the definition of a Hoare triple for partial correctness:$${\\varphi} P {ψ}$$This states that if the program P successfully executes from a starting state satisfying $\\varphi$, the result state will satisfy $ψ$. Observe that this is a safety property. Verifying Partial Correctness Given a transition diagram (L,T,s,t): Associate with each location $l ∈ L$ an assertion $Q(l) : Σ → B$. Prove that this assertion network is inductive, that is: For each transition in T $l_i \\xrightarrow[]{\\text{g;f}} l_j$ show that:$$Q(l_i) ∧ g ⇒ Q(l_j) ◦ f$$ Show that $\\varphi ⇒ Q(s)$ and $Q(t) ⇒ ψ$. Adding ConcurrencyParallel Composition Given two processes P and Q with transition diagrams (LP,TP,sP,tP) and (LQ,TQ,sQ,tQ), the parallel composition of $P$ and $Q$, written $P || Q$ is defined as (L,T,s,t) where: $L = L_P × L_Q$ $s = s_Ps_Q $ $t = t_Pt_Q $ $p_iq_i\\xrightarrow[]{\\text{g;f}}p_jq_i ∈ T$ if $p_i \\xrightarrow[]{\\text{g;f}} p_j ∈ TP $ $p_iq_i \\xrightarrow[]{\\text{g;f}} p_iq_j ∈ T$ if $q_i \\xrightarrow[]{\\text{g;f}} q_j ∈ TQ$ is the parallel composition “associative” and “commutative”, that is to define the parallel composition of N processes, we can just iterate parallel composition of 2 processes and no matter how we do it, we arrive at the “same” result? Yes State Space Explosion Problem Then number of locations and transitions grows exponentially as the number of processes increases. We can only use Floyd’s method directly on the parallel composition (product) diagram in the most basic examples. Our Solution We will instead use a method that allows us to define only inductive assertion networks for P and Q individually, and, by proving some non-interference properties derive an inductive network for P || Q automatically. This means we won’t have to draw that large product diagram! Owicki-Gries MethodSteps To show ${ϕ} P || Q {ψ}$: Define local assertion networks P and Q for both processes. Show that they’re inductive. For each location $p ∈ L_P$, show that $P(p)$ is not falsified by any transition of Q. That is, for each $q \\xrightarrow[]{\\text{g;f}} q’ ∈ TQ$: $P(p) ∧ Q(q) ∧ g ⇒ P(p) ◦ f$ Vice versa for Q. Show that $ϕ ⇒ P(s_P) ∧ Q(s_Q)$ and $P(t_P) ∧ Q(t_Q) ⇒ ψ$. One Big InvariantImagine assertion network(s) where every assertion is the same: An invariant. Benefit: We don’t need to prove interference freedom — the local verification conditions already show that the invariant is preserved. Compeleteness criteriaProgressA simple way to view progress as a completeness criteria is that it says a path is complete iff it is infinite or if it ends in a state with no outgoing transitions. This rules out all paths which end in states where they could take an outgoing transition but don’t. Weak FairnessWeak and strong fairness are defined in terms of tasks. A task is a set of transitions. Weak fairness as a property of paths can be expressed by the LTL formulas$◻(◻(\\text{enabled}(t))⇒◊(\\text{taken}(t)))$for each task $t$. Here $\\text{enabled}(t)$ is an atomic proposition that is true if a transition in $t$ is enabled. Likewise $\\text{taken}(t)$ holds if $t$ is taken in that state. Given a process $P=a⋅P+b$, weak fairness would state that eventually $b$ must occur, as we would not be able to loop infinitely around $a$, never taking the (forever enabled) $b$ transition. Viewed as a completeness criterion, weak fairness rules out all traces which do not obey the property above. Note that it implies progress, because any trace that will eventually take a forever enabled transition will surely not be able to sit in a state without taking an available transition. Strong FairnessStrong fairness is similar to weak fairness, except that the property has one extra operator:$◻(◻◊(\\text{enabled}(t))⇒◊(\\text{taken}(t)))$This is saying that our task $t$ does not have to be forever enabled, just enabled infinitely often. This means a process can go away and come back. For example, the process $P=a⋅a⋅P+b$ would not eventually do b under weak fairness, as after one a the b transition is no longer enabled. However because the infinite a path has b available infinitely often, strong fairness would require that b is eventually taken. Viewed as a completeness criterion, this rules out all properties that don’t obey the property above. It can be proven in LTL that strong fairness implies weak fairness. JustnessJustness is a slightly harder concept to formalise. It is weaker than both strong and weak fairness, but stronger than progress. Essentially, it is a criterion that says that individual components should be able to make progress on their own. For example, imagine a system that consists of three components, one which always does a, another which always does b, and another which synchronises with a:$$A=a⋅A\\\\B=b⋅B\\\\C=\\bar a⋅A\\\\\\text{𝖲𝗒𝗌𝗍𝖾𝗆}=A|B|C$$Now, a valid trace of this system would be simply $bbbbb⋯$ forever. This would satisfy progress, as a transition is always being taken. However, the actions a and $\\bar a$ that could occur, and the communication $τ$ transition that could between A and C are never taken in this trace. Justness says that each component of the system will always make progress if their resources are available. In other words, B doing unrelated b moves should never prevent A and C from communicating. Viewed as a completeness criterion, this is stronger than progress, as it is essentially the same as progress except applied locally, rather than globally. It is weaker than weak fairness, as justness would not require that $A=a⋅A+b$ eventually does a b, as all transitions are from the same component (A). Scheduling","link":"/2020/08/09/Foundation%20of%20Concurrency/"},{"title":"建立自己的blog","text":"使用hexo在github page上超级简单快速的搭建一个blog。 Github Page首先，我们建立一个自己的github page。可以直接查看官网步骤。https://pages.github.com/ 建立一个新的repository 名为username.github.io， username的部分填入你的Github用户名或者组织名。 用户名部分必须要一模一样，否则不会成功。 使用终端： clone到本地： 1$ git clone https://github.com/username/username.github.io 建立index.html 123$ cd username.github.io$ echo \"Hello World\" &gt; index.html$ push到github 123$ git add --all$ git commit -m \"Initial commit\"$ git push -u origin master 访问你的主页 https://username.github.io 就可以了。 Hexo为了快速做出一个好看的博客，我们用hexo这样一个模板。hexo功能非常全面，使用也很简单，还有很多好看的主题。完成效果就是我现在这个网页。直接在github中搜索hexo，就会看到这个repo：https://github.com/hexojs/hexo跟着readme中的步骤：Install Hexo 1$ npm install hexo-cli -g Setup your blog 12$ hexo init blog$ cd blog Start the server 1$ hexo server Create a new post 1$ hexo new \"Hello Hexo\" Generate static files 1$ hexo generate Hexo Theme - Icarus默认的主题是一个名叫land live2d 看板娘Deploy其他的好用工具Enable math blockHexo emoji1npm install hexo-filter-emoji","link":"/2020/08/10/%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84blog/"},{"title":"Operating system","text":"There are some learning notes about UNSW COMP3231/9201. Main contents are based on lecture slides. Operating System OverviewRolesRole 1The Operating System is an Abstract Machine Extends the basic hardware with added functionality Provides high-level abstractions More programmer friendly Common core for all applications E.g. Filesystem instead of just registers on a disk controller It hides the details of the hardware Makes application code portable Role2The Operating System is a Resource Manager Responsible for allocating resources to users and processes Must ensure No Starvation Progress Allocation is according to some desired policy First-come, first-served; Fair share; Weighted fair share; limits (quotas), etc… Overall, that the system is efficiently used Structural (Implementation) View: the Operating System is the software Privileged mode.Operating System Kernel Portion of the operating system that is running in privileged mode Usually resident (stays) in main memory Contains fundamental functionality Whatever is required to implement other services Whatever is required to provide security Contains most-frequently used functions Also called the nucleus or supervisor The Operating System is Privileged Applications should not be able to interfere or bypass the operating system OS can enforce the “extended machine” OS can enforce its resource allocation policies Prevent applications from interfering with each other Privilege-less OS Some Embedded OSs have no privileged component e.g. PalmOS, Mac OS 9, RTEMS Can implement OS functionality, but cannot enforce it. All software runs together No isolation One fault potentially brings down entire system Operating System Software Fundamentally, OS functions the same way as ordinary computer software It is machine code that is executed (same machine instructions as application) It has more privileges (extra instructions and access) Operating system relinquishes control of the processor to execute other programs Reestablishes control after System calls Interrupts (especially timer interrupts) The Monolithic Operating System Structure Also called the “spaghetti nest” approach Everything is tangled up with everything else. Linux, Windows, …. However, some reasonable structure usually prevails Process And Threads Process Thread Also called a task or job Unit of execution Execution of an individual program Can be traced “Owner” of resources allocated for program execution list the sequence of instructions that execute Encompasses one or more threads Belongs to a process Executes within it Process CreationPrincipal events that cause process creation System initialization Foreground processes (interactive programs) Background processes Email server, web server, print server, etc. Called a daemon (unix) or service (Windows) Execution of a process creation system call by a running process • New login shell for an incoming ssh connection User request to create a new process. Initiation of a batch job Note: Technically, all these cases use the same system mechanism to create new processes. Process TerminationConditions which terminate processes Normal exit (voluntary) Error exit (voluntary) Fatal error (involuntary) Killed by another process (involuntary) Implementation of Processes A processes’ information is stored in a process control block (PCB) The PCBs form a process table Process/Thread States Some Transition Causing EventsRunning → Ready Voluntary Yield() End of timeslice Running → Blocked Waiting for input File, network, Waiting for a timer (alarm signal) Waiting for a resource to become available The Ready Queue The Thread Model – Separating execution from the environment. Local variables are per thread Allocated on the stack Global variables are shared between all threads Allocated in data section Concurrency control is an issue Dynamically allocated memory (malloc) can be global or local Program defined (the pointer can be global or local) Concurrency and SynchronisationCritical Region We can control access to the shared resource by controlling access to the code that accesses the resource. A critical region is a region of code where shared resources are accessed. Variables, memory, files, etc… Uncoordinated entry to the critical region results in a race condition Incorrect behaviour, deadlock, lost work,… Critical Regions Solutions We seek a solution to coordinate access to critical regions. Also called critical sections Conditions required of any solution to the critical region problem Mutual Exclusion: No two processes simultaneously in critical region No assumptions made about speeds or numbers of CPUs Progress No process running outside its critical region may block another process Bounded No process waits forever to enter its critical region A lock variable Mutual Exclusion by Taking Turns Works due to strict alternation Each process takes turns Cons Busy waiting Process must wait its turn even while the other process is doing something else. With many processes, must wait for everyone to have a turn Does not guarantee progress if a process no longer needs a turn. Poor solution when processes require the critical section at differing rates Mutual Exclusion by Disabling Interrupts Before entering a critical region, disable interrupts After leaving the critical region, enable interrupts Pros simple Cons Only available in the kernel Blocks everybody else, even with no contention Slows interrupt response time Does not work on a multiprocessor Hardware Support for mutual exclusion Test and set instruction Can be used to implement lock variables correctly Hardware guarantees that the instruction executes atomically. Test-and-Set Pros Simple (easy to show it’s correct) Available at user-level To any number of processors To implement any number of lock variables Cons Busy waits (also termed a spin lock) Consumes CPU Starvation is possible when a process leaves its critical section and more than one process is waiting. Tackling the Busy-Wait Problem Sleep / Wakeup The idea When process is waiting for an event, it calls sleep to block, instead of busy waiting. The event happens, the event generator (another process) calls wakeup to unblock the sleeping process. Waking a ready/running process has no effect. Semaphores Dijkstra (1965) introduced two primitives that are more powerful than simple sleep and wakeup alone. P(): proberen, from Dutch to test. V(): verhogen, from Dutch to increment. Also called wait &amp; signal, down &amp; up. Semaphore Implementation If a resource is not available, the corresponding semaphore blocks any process waiting for the resource Blocked processes are put into a process queue maintained by the semaphore (avoids busy waiting!) When a process releases a resource, it signals this by means of the semaphore Signalling resumes a blocked process if there is any Wait and signal operations cannot be interrupted Complex coordination can be implemented by multiple semaphores Define a semaphore as a record 1234typedef struct { int count; struct process *L; } semaphore; Assume two simple operations: sleep suspends the process that invokes it. wakeup(P) resumes the execution of a blocked process P. Semaphore operations now defined as wait(S): 12345S.count--;if (S.count &lt; 0) { add this process to S.L; sleep; } signal(S): 12345S.count++;if (S.count &lt;= 0) { remove a process P from S.L; wakeup(P); } Each primitive is atomic E.g. interrupts are disabled for each Monitors To ease concurrent programming, Hoare (1974) proposed monitors. A higher level synchronisation primitive Programming language construct Idea A set of procedures, variables, data types are grouped in a special kind of module, a monitor. Variables and data types only accessed from within the monitor Only one process/thread can be in the monitor at any one time Mutual exclusion is implemented by the compiler (which should be less error prone) Condition Variable To allow a process to wait within the monitor, a condition variable must be declared, as ​ condition x, y; Condition variable can only be used with the operations wait and signal. The operation x.wait(); means that the process invoking this operation is suspended until another process invokes Another thread can enter the monitor while original is suspended x.signal(); The x.signal operation resumes exactly one suspended process. If no process is suspended, then the signal operation has no effect. DeadlocksResources Processes need access to resources in reasonable order Preemptable resources can be taken away from a process with no ill effects Nonpreemptable resources will cause the process to fail if taken away Introduction to DeadlocksFormal definition : A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause None of the processes can … run release resources be awakened Four Conditions for Deadlock Mutual exclusion condition each resource assigned to 1 process or is available Hold and wait condition process holding resources can request additional No preemption condition previously granted resources cannot be forcibly taken away Circular wait condition must be a circular chain of 2 or more processes each is waiting for resource held by next member of the chain Stratigies for dealing with Deadlocks just ignore the problem altogether prevention negating one of the four necessary conditions detection and recovery dynamic avoidance careful resource allocation Approach 1: The Ostrich Algorithm Pretend there is no problem Reasonable if deadlocks occur very rarely cost of prevention is high It’s a trade off between Convenience (engineering approach) Correctness (mathematical approach) Approach 2: Deadlock Prevention Resource allocation rules prevent deadlock by prevent one of the four conditions required for deadlock from occurring Mutual exclusion Not feasible in general • Some devices/resource are intrinsically not shareable. Hold and wait Require processes to request resources before starting =》a process never has to wait for what it needs Issues: may not know required resources at start of run =&gt; not always possible also ties up resources other processes could be using Variations: process must give up all resources if it would block holding a resource then request all immediately needed prone to livelock Livelocked processes are not blocked, change state regularly, but never make progress. No preemption Take resouces away This is not a viable option Circular Wait Numerically ordered resources Resources ordering is a common technique in practice Approach 3: Detection and Recovery Need a method to determine if a system is deadlocked. Assuming deadlocked is detected, we need a method of recovery to restore progress to the system. We need an approach for dealing with resources that consist of more than a single unit. Invariant: Sum of current resource allocation + resources available = resources that exist Detection Algorithm Look for an unmarked process Pi, for which the i-th row of R is less than or equal to A If found, add the i-th row of C to A, and mark Pi. Go to step 1 If no such process exists, terminate. Remaining processes are deadlocked Algorithm terminates with no unmarked processes We have no dead lock Recovery from Deadlock Recovery through preemption take a resource from some other process depends on nature of the resource Recovery through rollback checkpoint a process periodically use this saved state restart the process if it is found deadlocked No guarantee is won’t deadlock again Recovery through killing processes crudest but simplest way to break a deadlock kill one of the processes in the deadlock cycle the other processes get its resources choose process that can be rerun from the beginning Approach 4 Deadlock Avoidanceonly if enough information is available in advance. Safe and Unsafe States A state is safe if The system is not deadlocked There exists a scheduling order that results in every process running to completion, even if they all request their maximum resources immediately Unsafe states are not necessarily deadlocked With a lucky sequence, all processes may complete However, we cannot guarantee that they will complete (not deadlock) Safe states guarantee we will eventually complete all processes Deadlock avoidance algorithm Only grant requests that result in safe state StarvationA process never receives the resource it is waiting for, despite the resource (repeatedly) becoming free, the resource is always allocated to another waiting process. One solution: First-come, first-serve policy Process and Thread ImplementationFunction Stack FramesEach function call allocates a new stack frame for local variables, the return address, previous frame pointer etc. Frame pointer: start of current stack frame Stack pointer: end of current stack frame Process Structure Minimally consist of three segments Text contains the code (instructions) Data Global variables Stack Activation records of procedure/function/method Local variables Note: data can dynamically grow up E.g., malloc()-ing The stack can dynamically grow down E.g., increasing function call depth or recursion Processes User-mode Processes (programs) scheduled by the kernel Isolated from each other No concurrency issues between each other System-calls transition into and return from the kernel Kernel-mode Nearly all activities still associated with a process Kernel memory shared between all processes Concurrency issues exist between processes concurrently executing in a system call The Thread Model Items shared by all threads in a process Items that exist per thread Each thread has its own stack Implementing Threads in User SpaceUser-level threads implemented in a library Implementation at user-level User-level Thread Control Block (TCB), ready queue, blocked queue, and dispatcher Kernel has no knowledge of the threads (it only sees a single process) If a thread blocks waiting for a resource held by another thread inside the same process, its state is saved and the dispatcher switches to another ready thread Thread management (create, exit, yield, wait) are implemented in a runtime support library Pros Thread management and switching at user level is much faster than doing it in kernel level No need to trap (take syscall exception) into kernel and back to switch Dispatcher algorithm can be tuned to the application E.g. use priorities Can be implemented on any OS (thread or non-thread aware) Can easily support massive numbers of threads on a per-application basis Use normal application virtual memory Kernel memory more constrained. Difficult to efficiently support wildly differing numbers of threads for different applications. Cons Threads have to yield() manually (no timer interrupt delivery to userlevel) Co-operative multithreading A single poorly design/implemented thread can monopolise the available CPU time There are work-arounds (e.g. a timer signal per second to enable preemptive multithreading), they are course grain and a kludge. Does not take advantage of multiple CPUs (in reality, we still have a single threaded process as far as the kernel is concerned) If a thread makes a blocking system call (or takes a page fault), the process (and all the internal threads) blocks • Can’t overlap I/O with computation Kernel-provided Threads Also called kernel-level threads Even though they provide threads to applications Threads are implemented by the kernel TCBs are stored in the kernel A subset of information in a traditional PCB The subset related to execution context TCBs have a PCB associated with them Resources associated with the group of threads (the process) Thread management calls are implemented as system calls E.g. create, wait, exit Cons Thread creation and destruction, and blocking and unblocking threads requires kernel entry and exit. More expensive than user-level equivalent Pros Preemptive multithreading Parallelism Can overlap blocking I/O with computation Can take advantage of a multiprocessor Context Switch TerminologyA context switch can refer to A switch between threads Involving saving and restoring of state associated with a thread A switch between processes Involving the above, plus extra state associated with a process. E.g. memory maps Context Switch OccurrenceA switch between process/threads can happen any time the OS is invoked On a system call • Mandatory if system call blocks or on exit(); On an exception • Mandatory if offender is killed On an interrupt • Triggering a dispatch is the main purpose of the timer interrupt A thread switch can happen between any two instructions Note instructions do not equal program statements Context Switch Context switch must be transparent for processes/threads When dispatched again, process/thread should not notice that something else was running in the meantime (except for elapsed time) OS must save all state that affects the thread This state is called the process/thread context Switching between process/threads consequently results in a context switch. System Calls Can be viewed as special function calls Provides for a controlled entry into the kernel While in kernel, they perform a privileged operation Returns to original caller with the result The system call interface represents the abstract machine provided by the operating system. A Simple Model of CPU ComputationThe fetch-execute cycle Load memory contents from address in program counter (PC) The instruction Execute the instruction Increment PC Repeat Privileged-mode Operation To protect operating system execution, two or more CPU modes of operation exist Privileged mode User-mode All instructions are available Uses ‘safe’ subset of the instruction set - Only affects the state of the application itself - They cannot be used to uncontrollably interfere with OS All registers are available Only ‘safe’ registers are accessible System Call Mechanism Overview Processor mode Switched from user-mode to kernel-mode Switched back when returning to user mode Stack Pointer (SP) User-level SP is saved and a kernel SP is initialised User-level SP restored when returning to user-mode Program Counter (PC) User-level PC is saved and PC set to kernel entry point User-level PC restored when returning to user-level Kernel entry via the designated entry point must be strictly enforced Registers Set at user-level to indicate system call type and its arguments A convention between applications and the kernel Some registers are preserved at user-level or kernel-level in order to restart user-level execution Depends on language calling convention etc. Result of system call placed in registers when returning to user-level Another convention Computer Hardware Review (Memory Hierarchy) CPU Cache CPU cache is fast memory placed between the CPU and main memory Holds recently used data or instructions to save memory accesses. Matches slow RAM access time to CPU speed if high hit rate Is hardware maintained and (mostly) transparent to software Effective Access Time$$\\begin{align}T_{eff} &amp;= H \\times T_1 + (1-H)\\times T_2\\\\T_1 &amp;= \\text{access time of memory 1}\\\\T_2 &amp;= \\text{access time of memory 2}\\\\H&amp;=\\text{hit rate in memory 1}\\\\T_{eff}&amp;=\\text{effective access time of system}\\end{align}$$ A OS approach to improving system performance?A Strategy: Avoid Waiting for Disk Access Keep a subset of the disk’s data in main memory ⇒ OS uses main memory as a cache of disk contents A Strategy: Avoid Waiting for Internet Access Keep a subset of the Internet’s data on disk ⇒ Application uses disk as a cache of the Internet File ManagementFile Structure Sequence of Bytes OS considers a file to be unstructured Applications can impose their own structure Used by UNIX, Windows, most modern OSes File Types Regular files Directories Device Files –May be divided into Character Devices – stream of bytes Block Devices Some systems distinguish between regular file types –ASCII text files, binary files File Access Types (Patterns) Sequential access read all bytes/records from the beginning cannot jump around, could rewind or back up convenient when medium was magnetic tape Random access bytes/records read in any order essential for data base systems read can be … move file pointer (seek), then read or –lseek(location,…);read(…) each read specifies the file pointer –read(location,…) Executable Linkable Format (ELF) File system internalsUNIX storage stack FD table OF table VFS FS Buffer cache Disk scheduler Device driver Implementing File System The FS must map symbolic file names into a collection of block addresses The FS must keep track of – which blocks belong to which files. – in what order the blocks form the file – which blocks are free for allocation Given a logical region of a file, the FS must track the corresponding block(s) on disk. – Stored in file system metadata File Allocation Methods A file is divided into “blocks” – the unit of transfer to storage Contiguous Allocation✔ Easy bookkeeping (need to keep track of the starting block and length of the file) ✔ Increases performance for sequential operations ✗ Need the maximum size for the file at the time of creation ✗ As files are deleted, free space becomes divided into many small chunks (external fragmentation) Dynamic Allocation Strategies– Disk space allocated in portions as needed – Allocation occurs in fixed-size blocks ✔ No external fragmentation ✔ Does not require pre-allocating disk space ✗ Partially filled blocks (internal fragmentation) ✗ File blocks are scattered across the disk ✗ Complex metadata management (maintain the collection of blocks for each file) External and internal fragmentationExternal fragmentation – The space wasted external to the allocated memory regions – Memory space exists to satisfy a request but it is unusable as it is not contiguous Internal fragmentation – The space wasted internal to the allocated memory regions – Allocated memory may be slightly larger than requested memory; this size difference is wasted memory internal to a partition Dynamic allocation: Linked list allocation • Each block contains a pointer to the next block in the chain. Free blocks are also linked in a chain. ✔ Only single metadata entry per file ✔ Best for sequential files Linked list allocationEach block contains a pointer to the next block in the chain. Free blocks are also linked in a chain. ✔ Only single metadata entry per file ✔ Best for sequential files ✗ Poor for random access ✗ Blocks end up scattered across the disk due to free list eventually being randomised Dynamic Allocation: File Allocation Table (FAT)Keep a map of the entire FS in a separate table – A table entry contains the number of the next block of the file – The last block in a file and empty blocks are marked using reserved values The table is stored on the disk and is replicated in memory Random access is fast (following the in-memory list) File allocation table• Issues – Requires a lot of memory for large disks ​ • 200GB = 200*10^6 * 1K-blocks ==&gt; 200*10^6 FAT entries = 800MB – Free block lookup is slow ​ • searches for a free entry in table Dynamical Allocation: inode-based FS structure Idea: separate table (index-node or i-node) for each file. – Only keep table for open files in memory – Fast random access The most popular FS structure today i-node implementation issues i-nodes occupy one or several disk areas i-nodes are allocated dynamically, hence free-space management is required for i-nodes – Use fixed-size i-nodes to simplify dynamic allocation – Reserve the last i-node entry for a pointer (a block number) to an extension i-node. Free-space management – Approach 1: linked list of free blocks in free blocks on disk – Approach 2: keep bitmaps of free blocks and free i-nodes on disk Free block list• List of all unallocated blocks • Background jobs can re-order list for better contiguity • Store in free blocks themselves – Does not reduce disk capacity • Only one block of pointers need be kept in the main memory Bit tables• Individual bits in a bit vector flags used/free blocks • 16GB disk with 512-byte blocks –&gt; 4MB table • May be too large to hold in main memory • Expensive to search – Optimisations possible, e.g. a two level table • Concentrating (de)allocations in a portion of the bitmap has desirable effect of concentrating access • Simple to find contiguous free space Implementing directories• Directories are stored like normal files – directory entries are contained inside data blocks • The FS assigns special meaning to the content of these files – a directory file is a list of directory entries – a directory entry contains file name, attributes, and the file i-node number • maps human-oriented file name to a system-oriented name Fixed-size directory entries Variable-size directory entries – Either too small Freeing variable length entries can create external fragmentation in directory blocks – Or waste too much space Trade-off in FS block size Larger blocks require less FS metadata Smaller blocks waste less disk space (less internal fragmentation) Sequential Access – The larger the block size, the fewer I/O operations required Random Access – The larger the block size, the more unrelated data loaded. – Spatial locality of access improves the situation Choosing an appropriate block size is a compromise Virtual File System (VFS)Functionality Provides single system call interface for many file systems Transparent handling of network file systems File-based interface to arbitrary device drivers File-based interface to kernel data structures Provides an indirection layer for system calls VFS InterfaceTwo major data types – VFS ​ • Represents all file system types ​ • Contains pointers to functions to manipulate each file system as a whole (e.g. mount, unmount) ​ – Form a standard interface to the file system – Vnode ​ • Represents a file (inode) in the underlying filesystem ​ • Points to the real inode ​ • Contains pointers to functions to manipulate files/inodes (e.g. open, close, read, write,…) File Descriptors• File descriptors – Each open file has a file descriptor – Read/Write/lseek/…. use them to specify which file to operate on. • State associated with a file descriptor – File pointer • Determines where in the file the next read or write is performed – Mode • Was the file opened read-only, etc…. Per-Process fd table with global open file table•Per-process file descriptor array –Contains pointers to open file table entry •Open file table array –Contain entries with a fp and pointer to an vnode. •Provides –Shared file pointers if required –Independent file pointers if required •Example: –All three fds refer to the same file, two share a file pointer, one has an independent file pointer •Used by Linux and most other Unix operating systems Buffer Cache•Buffer: –Temporary storage used when transferring data between two entities •Especially when the entities work at different rates •Or when the unit of transfer is incompatible •Example: between application program and disk Buffering Disk Blocks Allow applications to work with arbitrarily sized region of a file –However, apps can still optimise for a particular block size Writes can return immediately after copying to kernel buffer –Avoids waiting until write to disk is complete –Write is scheduled in the background Can implement read-ahead by pre-loading next block on disk into kernel buffer –Avoids having to wait until next read is issued Cache–Fast storage used to temporarily hold data to speed up repeated access to the data Caching Disk BlocksOn access –Before loading block from disk, check if it is in cache first •Avoids disk accesses •Can optimise for repeated access for single or several processes Buffering and caching are related•Data is read into buffer; an extra independent cache copy would be wasteful •After use, block should be cached •Future access may hit cached copy •Cache utilises unused kernel memory space; ​ –may have to shrink, depending on memory demand Unix Buffer CacheOn read –Hash the device#, block# –Check if match in buffer cache –Yes, simply use in-memory copy –No, follow the collision chain –If not found, we load block from disk into buffer cache File System Consistency All modified blocks are written immediately to disk Generates much more disk traffic Temporary files written back Multiple updates not combined Used by DOS Gave okay consistency when » Floppies were removed from drives » Users were constantly resetting (or crashing) their machines Case Study: ext2 FSFeatures – Block size (1024, 2048, and 4096) configured at FS creation – inode-based FS – Performance optimisations to improve locality (from BSD FFS) Best and Worst Case Access PatternsAssume Inode already in memory To read 1 byte – Best: 1 access via direct block – Worst: 4 accesses via the triple indirect block To write 1 byte – Best: 1 write via direct block (with no previous content) – Worst: 4 reads (to get previous contents of block via triple indirect) + 1 write (to write modified block back) Worst Case Access Patterns with Unallocated Indirect Blocks Worst to write 1 byte – 4 writes (3 indirect blocks; 1 data) – 1 read, 4 writes (read-write 1 indirect, write 2; write 1 data) – 2 reads, 3 writes (read 1 indirect, read-write 1 indirect, write 1; write 1 data) – 3 reads, 2 writes (read 2, read-write 1; write 1 data) Worst to read 1 byte – If reading writes a zero-filled block on disk – Worst case is same as write 1 byte – If not, worst-case depends on how deep is the current indirect block tree. Inode SummaryThe inode (and indirect blocks) contains the on-disk metadata associated with a file Contains mode, owner, and other bookkeeping Efficient random and sequential access via indexed allocation Small files (the majority of files) require only a single access Larger files require progressively more disk accesses for random access Sequential access is still efficient Can support really large files via increasing levels of indirection Hard linksNote that inodes can have more than one name –Called a Hard Link Symbolic linksA symbolic link is a file that contains a reference to another file or directory Has its own inode and data block, which contains a path to the target file Marked by a special file attribute Transparent for some operations Can point across FS boundaries FS reliability e2fsck Scans the disk after an unclean shutdown and attempts to restore FS invariants Journaling file systems Keep a journal of FS updates Before performing an atomic update sequence, write it to the journal Replay the last journal entries upon an unclean shutdown – Example: ext3fs Case Study: ext3 FSDesign goals Add journaling capability to the ext2 FS Backward and forward compatibility with ext2 Existing ext2 partitions can be mounted as ext3 Leverage the proven ext2 performance Reuse most of the ext2 code base Reuse ext2 tools, including e2fsck Option1: Journal FS data structure updates✅ Efficient use of journal space; hence faster journaling ❌ Individual updates are applied separately ❌ The journaling layer must understand FS semantics Option2: Journal disk block updates❌ Even a small update adds a whole block to the journal ✅ Multiple updates to the same block can be aggregated into a single update ✅ The journaling layer is FSindependent (easier to implement) Ext3 implements Option 2 Journaling Block Device (JBD) The ext3 journaling layer is called Journaling Block Device (JBD) JBD interface Start a new transaction Update a disk block as part of a transaction Complete a transaction Completed transactions are buffered in RAM Commit: write transaction data to the journal (persistent storage) Multiple FS transactions are committed in one go Checkpoint: flush the journal to the disk Used when the journal is full or the FS is being unmounted JBD can keep the journal on a block device or in a file Enables compatibility with ext2 (the journal is just a normal file) JBD is independent of ext3-specific data structures Separation of concerns The FS maintains on-disk data and metadata JBD takes care of journaling Code reuse JBD can be used by any other FS that requires journaling Journaling modes Ext3 supports two journaling modes Metadata+data Enforces atomicity of all FS operations Metadata journaling Metadata is journalled Data blocks are written directly to the disk Improves performance Enforces file system integrity Does not enforce atomicity of write’s New file content can be stale blocks Virtual MemoryMemory Management Unit (or TLB)Page-based VMVirtual Memory– Divided into equalsized pages – A mapping is a translation between • A page and a frame • A page and null – Mappings defined at runtime • They can change – Address space can have holes – Process does not have to be contiguous in physical memory Physical Memory– Divided into equal-sized frames Typical Address Space Layout• Stack region is at top, and can grow down • Heap has free space to grow up • Text is typically read-only • Kernel is in a reserved, protected, shared region A process may be only partially resident – Allows OS to store individual pages on disk – Saves memory for infrequently used data &amp; code Page Faults Referencing an invalid page triggers a page fault An exception handled by the OS Broadly, two standard page fault types Illegal Address (protection error) Signal or kill the process Page not resident Get an empty frame Load page from disk Update page (translation) table (enter frame #, set valid bit, etc.) Restart the faulting instruction Shared Pages• Private code and data – Each process has own copy of code and data – Code and data can appear anywhere in the address space • Shared code – Single copy of code shared between all processes executing it – Code must not be self modifying – Code must appear at same address in all processes Page Table Structure• Page table is (logically) an array of frame numbers – Index by page number • Each page-table entry (PTE) also has other bits PTE Attributes (bits)• Present/Absent bit – Also called valid bit, it indicates a valid mapping for the page • Modified bit – Also called dirty bit, it indicates the page may have been modified in memory • Reference bit – Indicates the page has been accessed • Protection bits – Read permission, Write permission, Execute permission – Or combinations of the above • Caching bit – Use to indicate processor should bypass the cache when accessing memory • Example: to access device registers or memory Address Translation• Every (virtual) memory address issued by the CPU must be translated to physical memory ​ – Every load and every store instruction ​ – Every instruction fetch • Need Translation Hardware • In paging system, translation involves replace page number with a frame number Page Tables– Page table is very large – Access has to be fast, lookup for every memory reference Page tables are implemented as data structures in main memory • Most processes do not use the full 4GB address space – e.g., 0.1 – 1 MB text, 0.1 – 10 MB data, 0.1 MB stack • We need a compact representation that does not waste space – But is still very fast to search • Three basic schemes – Use data structures that adapt to sparsity – Use data structures which only represent resident pages – Use VM techniques for page tables (details left to extended OS) Two-level Page Table• 2nd –level page tables representing unmapped pages are not allocated – Null in the top-level page table Inverted Page Table (IPT) “Inverted page table” is an array of page numbers sorted (indexed) by frame number (it’s a frame table). Algorithm Compute hash of page number Extract index from hash table Use this to index into inverted page table Match the PID and page number in the IPT entry If match, use the index value as frame # for translation If no match, get next candidate IPT entry from chain field If NULL chain entry  page fault Properties of IPTs IPT grows with size of RAM, NOT virtual address space Frame table is needed anyway (for page replacement, more later) Need a separate data structure for non-resident pages Saves a vast amount of space (especially on 64-bit systems) Used in some IBM and HP workstations Improving the IPT: Hashed Page TableMultiprocessor SystemsAmdahl’s lawGiven a proportion P of a program that can be made parallel, and the remaining serial portion (1-P), speedup by using N processors $\\frac{1}{(1-P)+\\frac{P}{N}}$ Types of Multiprocessors (MPs)• UMA MP (Uniform Memory Access) • Access to all memory occurs at the same speed for all processors. • NUMA MP (Non-uniform memory access) • Access to some parts of memory is faster for some processors than other parts of memory Bus Based UMA Simplest MP is more than one processor on a single bus connect to memory Bus bandwidth becomes a bottleneck with more than just a few CPUs Each processor has a cache to reduce its need for access to memory Hope is most accesses are to the local cache Bus bandwidth still becomes a bottleneck with many CPUs With only a single shared bus, scalability can be limited by the bus bandwidth of the single bus Caching only helps so much Alternative bus architectures do exist. They improve bandwidth available Don’t eliminate constraint that bandwidth is limited Multi-core Processor(Multi-core) share the same Bus interface Multiprocessors can Increase computation power beyond that available from a single CPU Share resources such as disk and memory However Assumes parallelizable workload to be effective Assumes not I/O bound Shared buses (bus bandwidth) limits scalability Can be reduced via hardware design Can be reduced by carefully crafted software behaviour Good cache locality together with limited data sharing where possible How do we construct an OS for a multiprocessor?Each CPU has its own OS? Statically allocate physical memory to each CPU Each CPU runs its own independent OS Share peripherals Each CPU (OS) handles its processes system calls Used in early multiprocessor systems to ‘get them going’ Simpler to implement Avoids CPU-based concurrency issues by not sharing Scales – no shared serial sections Modern analogy, virtualisation in the cloud. Issues Each processor has its own scheduling queue We can have one processor overloaded, and the rest idle Each processor has its own memory partition We can a one processor thrashing, and the others with free memory No way to move free memory from one OS to another Symmetric Multiprocessors (SMP) OS kernel run on all processors Load and resource are balance between all processors Including kernel execution Issue: Real concurrency in the kernel Need carefully applied synchronisation primitives to avoid disaster Better alternative: identify largely independent parts of the kernel and make each of them their own critical section Allows more parallelism in the kernel Issue: Difficult task Code is mostly similar to uniprocessor code Hard part is identifying independent parts that don’t interfere with each other Remember all the inter-dependencies between OS subsystems. Lock contention can limit overall system performance Test-and-SetHardware guarantees that the instruction executes atomically on a CPU. Atomically: As an indivisible unit. The instruction can not stop half way through Test-and-Set on SMPIt does not work without some extra hardware support A solution: Hardware blocks all other CPUs from accessing the bus during the TSL instruction to prevent memory accesses by any other CPU. TSL has mutually exclusive access to memory for duration of instruction. Test-and Set is a busy-wait synchronisation primitive • Called a spinlock Issue: Lock contention leads to spinning on the lock Spinning on a lock requires blocking the bus which slows all other CPUs down Independent of whether other CPUs need a lock or no Caching does not help reduce bus contention Either TSL still blocks the bus Or TSL requires exclusive access to an entry in the local cache Reducing Bus Contention Read before TSL Spin reading the lock variable waiting for it to change When it does, use TSL to acquire the lock Allows lock to be shared read-only in all caches until its released no bus traffic until actual release No race conditions, as acquisition is still with TSL. Test and set performs poorly once there is enough CPUs to cause contention for lock Expected Read before Test and Set performs better Performance less than expected Still significant contention on lock when CPUs notice release and all attempt acquisition Critical section performance degenerates Critical section requires bus traffic to modify shared structure Lock holder competes with CPU that’s waiting as they test and set, so the lock holder is slower Slower lock holder results in more contention Cache Consistency Cache consistency is usually handled by the hardware. Spinning versus Blocking and Switching Spinning (busy-waiting) on a lock makes no sense on a uniprocessor The was no other running process to release the lock Blocking and (eventually) switching to the lock holder is the only sensible option. On SMP systems, the decision to spin or block is not as clear. The lock is held by another running CPU and will be freed without necessarily switching away from the requestor Spinning versus Switching Blocking and switching to another process takes time Save context and restore another Cache contains current process not new process Adjusting the cache working set also takes time TLB is similar to cache Switching back when the lock is free encounters the same again Spinning wastes CPU time directly Trade off If lock is held for less time than the overhead of switching to and back =》It’s more efficient to spin =〉Spinlocks expect critical sections to be short =》No waiting for I/O within a spinlock =〉No nesting locks within a spinlock Preemption and Spinlocks Critical sections synchronised via spinlocks are expected to be short Avoid other CPUs wasting cycles spinning What happens if the spinlock holder is preempted at end of holder’s timeslice Mutual exclusion is still guaranteed Other CPUs will spin until the holder is scheduled again!!!!! =》Spinlock implementations disable interrupts in addition to acquiring locks to avoid lock-holder preemption SchedulingThe scheduler decides who to run next. This process is sheduling Application behaviourBursts of CPU usage alternate with periods of I/O wait a) CPU-Bound process Spends most of its computing Time to completion largely determined by received CPU time b) I/O-Bound process Spend most of its time waiting for I/O to complete Small bursts of CPU to process I/O and request next I/O Time to completion largely determined by I/O request time We need a mix of CPU-bound and I/O-bound processes to keep both CPU and I/O systems busy Process can go from CPU- to I/O-bound (or vice versa) in different phases of execution Key InsightsChoosing to run an I/O-bound process delays a CPU-bound process by very little Choosing to run a CPU-bound process prior to an I/O-bound process delays the next I/O request significantly No overlap of I/O waiting with computation Results in device (disk) not as busy as possible Generally, favour I/O-bound processes over CPU-bound processes Generally, a scheduling decision is required when a process (or thread) can no longer continue, or when an activity results in more than one ready process. Preemptive versus Non-preemptive SchedulingNon-preemptive Once a thread is in the running state, it continues until it completes, blocks on I/O, or voluntarily yields the CPU A single process can monopolised the entire system Preemptive Scheduling (responsive system) Current thread can be interrupted by OS and moved to ready state. Usually after a timer interrupt and process has exceeded its maximum run time Can also be as a result of higher priority process that has become ready (after I/O interrupt). Ensures fairer service as single thread can’t monopolise the system Requires a timer interrupt Categories of Scheduling AlgorithmsBatch Systems • No users directly waiting, can optimise for overall machine performance Interactive Systems • Users directly waiting for their results, can optimise for users perceived performance Realtime Systems • Jobs have deadlines, must schedule such that all jobs (predictably) meet their deadlines Goals of Scheduling AlgorithmsAll Algorithms Fairness • Give each process a fair share of the CPU Policy Enforcement • What ever policy chosen, the scheduler should ensure it is carried out Balance/Efficiency • Try to keep all parts of the system busy Interactive Algorithms Minimise response time Response time is the time difference between issuing a command and getting the result – E.g selecting a menu, and getting the result of that selection Response time is important to the user’s perception of the performance of the system. Provide Proportionality Proportionality is the user expectation that short jobs will have a short response time, and long jobs can have a long response time. Generally, favour short jobs Real-time Algorithms Must meet deadlines Each job/task has a deadline. A missed deadline can result in data loss or catastrophic failure – Aircraft control system missed deadline to apply brakes Provide Predictability For some apps, an occasional missed deadline is okay – E.g. DVD decoder Predictable behaviour allows smooth DVD decoding with only rare skips Interactive schedulingRound Robin Scheduling Each process is given a timeslice to run in When the timeslice expires, the next process preempts the current process, and runs for its timeslice, and so on The preempted process is placed at the end of the queue Implemented with A ready queue A regular timer interrupt Pros – Fair, easy to implement Con – Assumes everybody is equal – Too short • Waste a lot of time switching between processes • Example: timeslice of 4ms with 1 ms context switch = 20% round robin overhead – Too long • System is not responsive • Example: timeslice of 100ms – If 10 people hit “enter” key simultaneously, the last guy to run will only see progress after 1 second. • Degenerates into FCFS if timeslice longer than burst length Priorities Each Process (or thread) is associated with a priority Provides basic mechanism to influence a scheduler decision: Scheduler will always chooses a thread of higher priority over lower priority Priorities can be defined internally or externally Internal: e.g. I/O bound or CPU bound External: e.g. based on importance to the user Usually implemented by multiple priority queues, with round robin on each queue Con Low priorities can starve Need to adapt priorities periodically Based on ageing or execution history Traditional UNIX Scheduler Two-level scheduler High-level scheduler schedules processes between memory and disk Low-level scheduler is CPU scheduler Based on a multilevel queue structure with round robin at each level The highest priority (lower number) is scheduled Priorities are re-calculated once per second, and re-inserted in appropriate queue – Avoid starvation of low priority threads – Penalise CPU-bound threads Priority = CPU_usage +nice +base CPU_usage = number of clock ticks Nice is a value given to the process by a user to permanently boost or reduce its priority ( Reduce priority of background jobs) Base is a set of hardwired, negative values used to boost priority of I/O bound system activities Single Shared Ready Queue Pros – Simple – Automatic load balancing Cons – Lock contention on the ready queue can be a major bottleneck ​ • Due to frequent scheduling or many CPUs or both – Not all CPUs are equal ​ • The last CPU a process ran on is likely to have more related entries in the cache Affinity Scheduling Basic Idea – Try hard to run a process on the CPU it ran on last time One approach: Multiple Queue Multiprocessor Scheduling Multiple Queue SMP Scheduling Each CPU has its own ready queue Coarse-grained algorithm assigns processes to CPUs – Defines their affinity, and roughly balances the load The bottom-level fine-grained scheduler: – Is the frequently invoked scheduler (e.g. on blocking on I/O, a lock, or exhausting a timeslice) – Runs on each CPU and selects from its own ready queue Ensures affinity – If nothing is available from the local ready queue, it runs a process from another CPUs ready queue rather than go idle Termed “Work stealing” Pros – No lock contention on per-CPU ready queues in the (hopefully) common case – Load balancing to avoid idle queues – Automatic affinity to a single CPU for more cache friendly behaviour","link":"/2020/08/11/Operating%20System/"}],"tags":[{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"blog page","slug":"blog-page","link":"/tags/blog-page/"}],"categories":[{"name":"notes","slug":"notes","link":"/categories/notes/"}]}